{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Automated ML with azureml\n",
    "\n",
    "The dependencies are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Dataset, Datastore, Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "We will try to predict the rating of modified version of the **Kaggle Trip advisor dataset**.\n",
    "\n",
    "The Dataset contains a Trip Advisor hotel review text column as well as a Rating column with Ratings from 0 - 5 stars. \n",
    "\n",
    "> The Tripadvisor Hotel Review Dataset file, is derived from the publication: \n",
    ">\n",
    ">_Alam, M. H., Ryu, W.-J., Lee, S., 2016. Joint multi-grain topic senti- ment: modeling semantic aspects for online >reviews. Information Sciences 339, 206â€“223._ \n",
    ">\n",
    "> You can download the Dataset with the link:\n",
    "> [trip-advisor-hotel-reviews](https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews)\n",
    "\n",
    "In the original Dataset the target **Rating** column contains the values 0* - 5*.\n",
    "\n",
    "In a modified version of the dataset we will try to predict the **norm_rating** column based on the **Review** text column as a **classification task** with:\n",
    "\n",
    "* class 0 - Negative reviews (1* & 2* rating)\n",
    "* class 1 - Neutral reviews (3* rating)\n",
    "* class 2 - Positive reviews (4* & 5* rating)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Workspace and create an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'automl_review_classifier'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset and perform a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filepath_2_dataset = r\"C:\\Users\\christoph.hiemenz\\Desktop\\Grid_search_results\\hotel_reviews_featurized_roberta.csv\"\n",
    "# Read the Dataset as a pandas dataframe\n",
    "hotel_review_dataset = pd.read_csv(filepath_2_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First the same train test split is performed for the Dataset to make it available to both AutoML and Hyperdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (16392, 808)\n",
      "X_test: (4099, 808)\n",
      "y_train: 16392\n",
      "y_test: 4099\n"
     ]
    }
   ],
   "source": [
    "# Get hotel review text and normalized rating\n",
    "X = hotel_review_dataset.drop(columns=['text'])\n",
    "y = list(hotel_review_dataset.norm_rating)\n",
    "X_train, X_test, y_train, y_test = train_test_split(hotel_review_dataset, y, test_size=0.2, random_state=42)\n",
    "print(f\"X_train: {X_train.shape}\\nX_test: {X_test.shape}\\ny_train: {len(y_train)}\\ny_test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training set and test sets will be registered separately to ensure strict separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-ba8989840918>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['norm_rating'] = y_train\n",
      "<ipython-input-26-ba8989840918>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['norm_rating'] = y_test\n"
     ]
    }
   ],
   "source": [
    "X_train['norm_rating'] = y_train\n",
    "X_test['norm_rating'] = y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The AutoML train/testsets should contain just the text column and norm rating column (no feature engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the different train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10726    0\n",
       "14919    2\n",
       "19098    1\n",
       "2450     2\n",
       "960      2\n",
       "        ..\n",
       "1576     1\n",
       "18714    2\n",
       "12690    2\n",
       "18095    2\n",
       "11836    2\n",
       "Name: norm_rating, Length: 4099, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_automl = X_train.loc[:, ['text', 'norm_rating']]\n",
    "X_test_automl = X_test.loc[:, ['text', 'norm_rating']]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Upload the training/test data in the default datastore\n",
    "train_dataset_path_automl = \"data/train_set_automl.csv\"\n",
    "X_train_automl.to_csv(train_dataset_path_automl)\n",
    "test_dataset_path_automl = \"data/test_set_automl.csv\"\n",
    "X_test_automl.to_csv(test_dataset_path_automl)\n",
    "\n",
    "train_dataset_path = \"data/train_set.csv\"\n",
    "X_train.to_csv(train_dataset_path)\n",
    "test_dataset_path = \"data/test_set.csv\"\n",
    "X_test.to_csv(test_dataset_path)\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir=\"data\", target_path=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training and test Datasets and register them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training = Dataset.Tabular.from_delimited_files(path = [(datastore, (\"data/train_set_automl.csv\"))])\n",
    "dataset_training = dataset_training.register(workspace=ws, name=\"auto-ml-training-data\", description=\"Hotel Review AutoML Training Data\")\n",
    "\n",
    "dataset_test =  Dataset.Tabular.from_delimited_files(path = [(datastore, (\"data/test_set_automl.csv\"))])\n",
    "dataset_test = dataset_training.register(workspace=ws, name=\"auto-ml-test-data\", description=\"Hotel Review AutoML Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Compute Target for AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a Compute Target for AutoML\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cpu_cluster_name = \"cpu-cluster-1\"\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print(\"Found existing Compute Target\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size = \"Standard_D2_V2\", max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "* _experiment_timeout_minutes_: was set to prevent the experiment from running for long timer periods with high cost\n",
    "* _max_concurrent_iterations_: was set to 4 since only 4 compute target nodes are available for paralle child runs\n",
    "* _primary_metric_: was set to AUC_weighted since this includes a balance between false positive and true positive rate\n",
    "* _n_cross_validations_: 5 crossvalidations were selected, since this results in a more robust mean/std estimation for each model\n",
    "\n",
    "* _enable_early_stopping_: to prevent unproductive runs which lead to no improvement and costs\n",
    "* _compute_target_: needs to be define to perform the AutoML computations\n",
    "* _task_: needs to be classification since the label column is defining separate classes\n",
    "* _training_data_: corresponds to the training set\n",
    "* _label_column_: corresponds to the target/label column defining the separate classes\n",
    "* _debug_log_: defined to enable detailed logging of automl errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "## Define key AutoML Settings\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 20,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"primary_metric\": \"AUC_weighted\",\n",
    "    \"n_cross_validations\": 5\n",
    "}\n",
    "\n",
    "## Setup an AutoMLConfig object\n",
    "automl_config = AutoMLConfig(\n",
    "    compute_target=compute_target,\n",
    "    task=\"classification\",\n",
    "    training_data=dataset_training,\n",
    "    label_column_name=\"norm_rating\",\n",
    "    enable_early_stopping=True,\n",
    "    debug_log=\"automl_errors.log\",\n",
    "    **automl_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# The Experiment needs to be submitted in order to execute the AutoML run\n",
    "automl_pipeline_run = experiment.submit(automl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "Write about the different models trained and their performance. Why do you think some models did better than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(automl_pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "automl_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics and Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the performance metrics for the AutoML run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_output = automl_pipeline_run.get_pipeline_output(metrics_output_name)\n",
    "num_file_downloaded = metrics_output.download('.', show_progress=True)\n",
    "\n",
    "import json\n",
    "with open(metrics_output._path_on_datastore) as f:\n",
    "    metrics_output_result = f.read()\n",
    "    \n",
    "deserialized_metrics_output = json.loads(metrics_output_result)\n",
    "df = pd.DataFrame(deserialized_metrics_output)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best model and the best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = automl_pipeline_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_file_names())\n",
    "best_model = best_run.register_model(workspace=ws, model_name=\"best-automl-model\", model_path=\"outputs/automl_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run, fitted_model = automl_pipeline_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the best model\n",
    "dataset_test = Dataset.Tabular.from_delimited_files(path='https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv')\n",
    "df_test = dataset_test.to_pandas_dataframe()\n",
    "\n",
    "y_test = df_test['norm_rating']\n",
    "X_test = df_test.drop(['norm_rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "ypred = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, ypred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create the environment\n",
    "myenv = Environment(name=\"myenv\")\n",
    "conda_dep = CondaDependencies()\n",
    "\n",
    "# Define the packages needed by the model and scripts\n",
    "conda_dep.add_conda_package(\"pandas\")\n",
    "conda_dep.add_conda_package(\"numpy\")\n",
    "conda_dep.add_conda_package(\"scikit-learn\")\n",
    "# You must list azureml-defaults as a pip dependency\n",
    "conda_dep.add_pip_package(\"azureml-defaults\")\n",
    "\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "myenv.python.conda_dependencies=conda_dep\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\",\n",
    "                                   environment=myenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "service_name = 'automl-review-classification'\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[best_model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=aci_config,\n",
    "                       overwrite=True)\n",
    "service.wait_for_deployment(show_output=True)\n",
    "print(\"scoring URI: \" + service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# Get a token to authenticate to the compute instance from remote\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "\n",
    "# Create and submit a request using the auth header\n",
    "headers = auth_header\n",
    "# Add content type header\n",
    "headers.update({'Content-Type':'application/json'})\n",
    "\n",
    "# Sample data to send to the service\n",
    "test_sample = json.dumps({'data': [\n",
    "    [1,2,3,4,5,6,7,8,9,10],\n",
    "    [10,9,8,7,6,5,4,3,2,1]\n",
    "]})\n",
    "test_sample = bytes(test_sample, encoding = 'utf8')\n",
    "\n",
    "# Replace with the URL for your compute instance, as determined from the previous section\n",
    "service_url = service.endpoint\n",
    "# for a compute instance, the url would be https://vm-name-6789.northcentralus.instances.azureml.net/score\n",
    "response = requests.post(service_url, test_sample, headers=headers)\n",
    "print(\"prediction:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(local_service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport os\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# SENTIMENT\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "SEQUENCE_LENGTH = 300\n",
    "\n",
    "# Called when the deployed service starts\n",
    "def init():\n",
    "    global model\n",
    "    global tokenizer\n",
    "    global encoder\n",
    "    global w2v_model\n",
    "\n",
    "    # Get the path where the deployed model can be found.\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), './models')\n",
    "    # load models\n",
    "    model = load_model(model_path + '/model.h5')\n",
    "    w2v_model = Word2Vec.load(model_path + '/model.w2v')\n",
    "\n",
    "    with open(model_path + '/tokenizer.pkl','rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    with open(model_path + '/encoder.pkl','rb') as handle:\n",
    "        encoder = pickle.load(handle)\n",
    "\n",
    "# Handle requests to the service\n",
    "def run(data):\n",
    "    try:\n",
    "        # Pick out the text property of the JSON request.\n",
    "        # This expects a request in the form of {\"text\": \"some text to score for sentiment\"}\n",
    "        data = json.loads(data)\n",
    "        prediction = predict(data['text'])\n",
    "        #Return prediction\n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error\n",
    "\n",
    "# Determine sentiment from score\n",
    "def decode_sentiment(score, include_neutral=True):\n",
    "    if include_neutral:\n",
    "        label = NEUTRAL\n",
    "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
    "            label = NEGATIVE\n",
    "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "            label = POSITIVE\n",
    "        return label\n",
    "    else:\n",
    "        return NEGATIVE if score < 0.5 else POSITIVE\n",
    "\n",
    "# Predict sentiment using the model\n",
    "def predict(text, include_neutral=True):\n",
    "    start_at = time.time()\n",
    "    # Tokenize text\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
    "    # Predict\n",
    "    score = model.predict([x_test])[0]\n",
    "    # Decode sentiment\n",
    "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
    "\n",
    "    return {\"label\": label, \"score\": float(score),\n",
    "       \"elapsed_time\": time.time()-start_at}  "
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
