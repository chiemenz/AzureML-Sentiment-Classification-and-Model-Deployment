{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Datastore, Workspace, Experiment\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from azureml.core.run import Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tabular training and test datasets are loaded from the datastore and are registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get workspace by name\n",
    "ws = Workspace.get(name=\"quick-starts-ws-132734\")\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "dataset_training = Dataset.Tabular.from_delimited_files(path = [(datastore, (\"data/train_set.csv\"))])\n",
    "dataset_training = dataset_training.register(workspace=ws, name=\"hyperdrive-training-data\", description=\"Hotel Review Hyperdrive Training Data\")\n",
    "\n",
    "dataset_test =  Dataset.Tabular.from_delimited_files(path = [(datastore, (\"data/test_set.csv\"))])\n",
    "dataset_test = dataset_training.register(workspace=ws, name=\"hyperdrive-test-data\", description=\"Hotel Review Hyperdrive Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The registered tabular train and test sets are converted to pandas and split into the feature matrix and label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset_training.to_pandas_dataframe()\n",
    "test_df = dataset_test.to_pandas_dataframe()\n",
    "\n",
    "x_train = train_df.drop(columns=['norm_rating']).to_numpy()\n",
    "y_train = list(train_df.norm_rating)\n",
    "x_test = test_df.drop(columns=['norm_rating']).to_numpy()\n",
    "y_test = list(test_df.norm_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the main method is defined for fitting the classifier and computing its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "run = Run.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Add arguments to script\n",
    "    parser = argparse.ArgumentParser(description=\"hyperparameters of the logistic regression model\")\n",
    "\n",
    "    parser.add_argument('--max-depth', type=int, default=3,\n",
    "                        help=\"How deep is the tree growing during one round of boosting\")\n",
    "    parser.add_argument('--min-child-weight', type=int,\n",
    "                        default=2,\n",
    "                        help=\"Minimum sum of weight for all observations in a child. Controls overfitting\")  \n",
    "    parser.add_argument('--gamma', type=float,\n",
    "                        default=0,\n",
    "                        help=\"Gamma corresponds to the minimum loss reduction required to make a split.\")\n",
    "    parser.add_argument('--subsample', type=float,\n",
    "                        default=0.9,\n",
    "                        help=\"What fraction of samples are randomly sampled per tree.\")\n",
    "    parser.add_argument('--colsample-bytree', type=float,\n",
    "                        default=0.8,\n",
    "                        help=\"What fraction of feature columns are randomly sampled per tree.\")\n",
    "    parser.add_argument('--reg-alpha', type=float,\n",
    "                        default=0.00001,\n",
    "                        help=\"L1 regularization of the weights. Increasing the values more strongly prevents overfitting.\")\n",
    "    parser.add_argument('--nthread', type=int,\n",
    "                        default=4,\n",
    "                        help=\"Number of parallel threads for XGBoost.\")\n",
    "    parser.add_argument('--eta', type=float,\n",
    "                        default=0.2,\n",
    "                        help=\"Learning rate for XGBoost.\")\n",
    "    parser.add_argument('--n-estimators', type=int,\n",
    "                        default=500,\n",
    "                        help=\"Number of XGBoost estimators.\")    \n",
    "    parser.add_argument('--seed', type=int,\n",
    "                        default=42,\n",
    "                        help=\"Random seed.\")\n",
    "        \n",
    "    args = parser.parse_args()\n",
    "    params = {\n",
    "                       'eta':args.eta,                       \n",
    "                       'n_estimators':args.n_estimators,\n",
    "                       'max_depth':args.max_depth,\n",
    "                       'min_child_weight': args.min_child_weight,\n",
    "                       'gamma': args.gamma,\n",
    "                       'subsample':args.subsample,\n",
    "                       'colsample_bytree': args.colsample_bytree,\n",
    "                       'reg_alpha': args.reg_alpha,\n",
    "                       'nthread':args.nthread,\n",
    "                       'seed':args.seed,\n",
    "                       'objective':'multi:softmax',\n",
    "                       'num_class': 3,\n",
    "                       }\n",
    "\n",
    "    \n",
    "    run.log(\"Regularization Strength:\", np.float(args.C))\n",
    "    run.log(\"Max iterations:\", np.int(args.max_iter))\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "                                objective =params['objective'],\n",
    "                                eta=params['eta'],\n",
    "                                max_depth = params['max_depth'],\n",
    "                                gamma=params['gamma'],\n",
    "                                n_estimators = params['n_estimators'],\n",
    "                                seed=params['seed'],\n",
    "                                nthread=params['nthread'],\n",
    "                                colsample_bytree=params['colsample_bytree'],\n",
    "                                reg_alpha=params['reg_alpha'],\n",
    "                                num_class=params['num_class']\n",
    "                             )\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, model.predict(x_test))\n",
    "\n",
    "    run.log(\"Accuracy\", np.float(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
